# ** EXAM OBJECTIVE: CLUSTER ADMINISTRATION **
# GOAL: Allocate the shards in a way that satisfies a given set of requirements
# REQUIRED SETUP: /

# Download the the exam version of Elasticsearch
# Deploy the cluster `eoc-06-cluster`, with three nodes named `node1`, `node2`, and `node3`
# Configure the Zen Discovery module of each node so that they can communicate with each other
# Connect a Kibana instance to `node3`
# Start the cluster
# Create the index `hamlet-1` with two primary shards and one replica
# Add some documents to `hamlet-1` by running the following _bulk command
PUT hamlet-1/_doc/_bulk
{"index":{"_index":"hamlet-1","_id":0}}
{"line_number":"1","speaker":"BERNARDO","text_entry":"Whos there?"}
{"index":{"_index":"hamlet-1","_id":1}}
{"line_number":"2","speaker":"FRANCISCO","text_entry":"Nay, answer me: stand, and unfold yourself."}
{"index":{"_index":"hamlet-1","_id":2}}
{"line_number":"3","speaker":"BERNARDO","text_entry":"Long live the king!"}
{"index":{"_index":"hamlet-1","_id":3}}
{"line_number":"4","speaker":"FRANCISCO","text_entry":"Bernardo?"}
{"index":{"_index":"hamlet-1","_id":4}}
{"line_number":"5","speaker":"BERNARDO","text_entry":"He."}
# Create the index `hamlet-2` with two primary shard and one replica
# Add some documents to `hamlet-2` by running the following _bulk command
PUT hamlet-2/_doc/_bulk
{"index":{"_index":"hamlet-2","_id":5}}
{"line_number":"6","speaker":"FRANCISCO","text_entry":"You come most carefully upon your hour."}
{"index":{"_index":"hamlet-2","_id":6}}
{"line_number":"7","speaker":"BERNARDO","text_entry":"Tis now struck twelve; get thee to bed, Francisco."}
{"index":{"_index":"hamlet-2","_id":7}}
{"line_number":"8","speaker":"FRANCISCO","text_entry":"For this relief much thanks: tis bitter cold,"}
{"index":{"_index":"hamlet-2","_id":8}}
{"line_number":"9","speaker":"FRANCISCO","text_entry":"And I am sick at heart."}
{"index":{"_index":"hamlet-2","_id":9}}
{"line_number":"10","speaker":"BERNARDO","text_entry":"Have you had quiet guard?"}
# Check that the replicas of indices `hamlet-1` and `hamlet-2` have been allocated
# Check the distribution of the primary shards and replicas of indices `hamlet-1` and `hamlet-2` across the nodes of the cluster
# Configure `hamlet-1` to allocate both primary shards to `node2`, using the node name
# Verify the success of the last action by using the _cat API
# Configure `hamlet-2` so that no primary shard is allocated to `node3`
# Verify the success of the last action by using the _cat API
# Remove any allocation filter setting associated with `hamlet-1` and `hamlet-2`
# Let's assume that we have deployed the `eoc-06-cluster` cluster across two availability zones, named `earth` and `mars`. Add the attribute `AZ` to the nodes configuration, and set its value to "earth" for `node1` and `node2`, and to "mars" for `node3`
# Restart the cluster
# Configure the cluster to force shard allocation awareness based on the two availability zones, and persist such configuration across cluster restarts
# Verify the success of the last action by using the _cat API
# Configure the cluster to reflect a hot/warm architecture, with `node1` as the only hot node
# Configure the `hamlet-1` index to allocate its shards only to warm nodes
# Verify the success of the last action by using the _cat API
# Remove the hot/warm shard filtering configuration from the `hamlet-1` configuration
# Let's assume that the nodes have either a "large" or "small" local storage. Add the attribute `storage` to the nodes configuration, and set its value so that `node2` is the only with a "small" storage
# Configure the `hamlet-2` index to allocate its shards only to nodes with a large storage size
# Verify the success of the last action by using the _cat API

# LAB STEPS
PUT /hamlet-1
{
    "settings" : {
        "index" : {
            "number_of_shards" : 2, 
            "number_of_replicas" : 1 
        }
    }
}
GET hamlet-1/_settings  

PUT hamlet-1/_doc/_bulk
{"index":{"_index":"hamlet-1","_id":0}}  
{"line_number":"1","speaker":"BERNARDO","text_entry":"Whos there?"}
{"index":{"_index":"hamlet-1","_id":1}} 
{"line_number":"2","speaker":"FRANCISCO","text_entry":"Nay, answer me: stand, and unfold yourself."}
{"index":{"_index":"hamlet-1","_id":2}}
{"line_number":"3","speaker":"BERNARDO","text_entry":"Long live the king!"}
{"index":{"_index":"hamlet-1","_id":3}}
{"line_number":"4","speaker":"FRANCISCO","text_entry":"Bernardo?"}
{"index":{"_index":"hamlet-1","_id":4}}
{"line_number":"5","speaker":"BERNARDO","text_entry":"He."}


GET _cat/shards?v

GET _cat/shards/hamlet-*?v

index    shard prirep state   docs store ip        node
hamlet-1 1     p      STARTED    0  230b 10.0.0.14 node3
hamlet-1 1     r      STARTED    0  230b 10.0.0.12 node1
hamlet-1 0     r      STARTED    0  230b 10.0.0.13 node2
hamlet-1 0     p      STARTED    0  230b 10.0.0.12 node1


GET /_shard_stores?status=green

PUT /hamlet-2
{
    "settings" : {
        "index" : {
            "number_of_shards" : 2, 
            "number_of_replicas" : 1 
        }
    }
}

PUT hamlet-2/_doc/_bulk
{"index":{"_index":"hamlet-2","_id":5}}
{"line_number":"6","speaker":"FRANCISCO","text_entry":"You come most carefully upon your hour."}
{"index":{"_index":"hamlet-2","_id":6}}
{"line_number":"7","speaker":"BERNARDO","text_entry":"Tis now struck twelve; get thee to bed, Francisco."}
{"index":{"_index":"hamlet-2","_id":7}}
{"line_number":"8","speaker":"FRANCISCO","text_entry":"For this relief much thanks: tis bitter cold,"}
{"index":{"_index":"hamlet-2","_id":8}}
{"line_number":"9","speaker":"FRANCISCO","text_entry":"And I am sick at heart."}
{"index":{"_index":"hamlet-2","_id":9}}
{"line_number":"10","speaker":"BERNARDO","text_entry":"Have you had quiet guard?"}

GET _cat/shards/hamlet-*?v

index    shard prirep state   docs store ip        node
hamlet-1 1     p      STARTED    2 4.8kb 10.0.0.14 node3
hamlet-1 1     r      STARTED    2 4.8kb 10.0.0.12 node1
hamlet-1 0     r      STARTED    3 5.2kb 10.0.0.13 node2
hamlet-1 0     p      STARTED    3 5.2kb 10.0.0.12 node1
hamlet-2 1     p      STARTED    1 4.7kb 10.0.0.13 node2
hamlet-2 1     r      STARTED    1 4.7kb 10.0.0.12 node1
hamlet-2 0     p      STARTED    4 5.6kb 10.0.0.14 node3
hamlet-2 0     r      STARTED    4 5.6kb 10.0.0.13 node2



# allocate primary shards to node2

GET _cat/shards/hamlet-1?v

index    shard prirep state   docs store ip        node
hamlet-1 1     p      STARTED    2 4.8kb 10.0.0.13 node2
hamlet-1 1     r      STARTED    2 4.8kb 10.0.0.12 node1
hamlet-1 0     p      STARTED    3 5.2kb 10.0.0.14 node3
hamlet-1 0     r      STARTED    3 5.2kb 10.0.0.13 node2

POST /_cluster/reroute
{
    "commands" : [
      {
          "cancel": {
                "index" : "hamlet-1", "shard" : 0,
                "node" : "node2"
          }
        },
        {
            "move" : {
                "index" : "hamlet-1", "shard" : 0,
                "from_node" : "node3", "to_node" : "node2"
            }
        }
        
    ]
}


GET _cat/shards/hamlet-1?v
index    shard prirep state   docs store ip        node
hamlet-1 1     p      STARTED    2 4.7kb 10.0.0.13 node2
hamlet-1 1     r      STARTED    2 4.7kb 10.0.0.12 node1
hamlet-1 0     p      STARTED    3 5.2kb 10.0.0.13 node2
hamlet-1 0     r      STARTED    3 5.2kb 10.0.0.12 node1

# no primary shards on node3

GET _cat/shards/hamlet-2?v
index    shard prirep state   docs store ip        node
hamlet-2 1     p      STARTED    1 4.7kb 10.0.0.13 node2
hamlet-2 1     r      STARTED    1 4.7kb 10.0.0.12 node1
hamlet-2 0     p      STARTED    4 5.6kb 10.0.0.14 node3
hamlet-2 0     r      STARTED    4 5.6kb 10.0.0.13 node2

POST /_cluster/reroute
{
    "commands" : [
      {
            "move" : {
                "index" : "hamlet-2", "shard" : 0,
                "from_node" : "node3", "to_node" : "node1"
            }
        }
    ]
}

GET _cat/shards/hamlet-2?v
index    shard prirep state   docs store ip        node
hamlet-2 1     p      STARTED    1 4.8kb 10.0.0.14 node3
hamlet-2 1     r      STARTED    1 4.7kb 10.0.0.12 node1
hamlet-2 0     r      STARTED    4 5.6kb 10.0.0.13 node2
hamlet-2 0     p      STARTED    4 5.7kb 10.0.0.12 node1

POST /_cluster/reroute
{
    "commands" : [
      {
            "move" : {
                "index" : "hamlet-2", "shard" : 1,
                "from_node" : "node3", "to_node" : "node2"
            }
        }
    ]
}

GET _cat/shards/hamlet-2?v
index    shard prirep state   docs store ip        node
hamlet-2 1     p      STARTED    1 4.8kb 10.0.0.13 node2
hamlet-2 1     r      STARTED    1 4.7kb 10.0.0.12 node1
hamlet-2 0     r      STARTED    4 5.7kb 10.0.0.14 node3
hamlet-2 0     p      STARTED    4 5.7kb 10.0.0.12 node1


# remove allocation filter setting
PUT hamlet-1/_settings
{
  "index.routing.allocation.include._name": null
}

PUT hamlet-2/_settings
{
  "index.routing.allocation.include._name": null
}

GET /hamlet-1/_settings

{
  "hamlet-1" : {
    "settings" : {
      "index" : {
        "creation_date" : "1584884249521",
        "number_of_shards" : "2",
        "number_of_replicas" : "1",
        "uuid" : "8-2zh2ADTHmoRfqfRhGThQ",
        "version" : {
          "created" : "7020199"
        },
        "provided_name" : "hamlet-1"
      }
    }
  }
}

# Configure the cluster to force shard allocation awareness based on the two availability zones, and persist such configuration across cluster restarts

add below config to elasticsearch.yml
node1, node2:
node.attr.az: earth

node3:
node.attr.az: mars

GET /_cat/nodeattrs?v
node  host      ip        attr            value
node3 10.0.0.14 10.0.0.14 az              mars
node3 10.0.0.14 10.0.0.14 xpack.installed true
node2 10.0.0.13 10.0.0.13 az              earth
node2 10.0.0.13 10.0.0.13 xpack.installed true
node1 10.0.0.12 10.0.0.12 az              earth
node1 10.0.0.12 10.0.0.12 xpack.installed true

PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.awareness.attributes": "az"
  }
}


PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.awareness.attributes": "az",
    "cluster.routing.allocation.awareness.force.az.values":["earth", "mars"]
  }
}

GET /_cluster/settings

{
  "persistent" : {
    "cluster" : {
      "routing" : {
        "allocation" : {
          "awareness" : {
            "attributes" : "az",
            "force" : {
              "az" : {
                "values" : [
                  "earth",
                  "mars"
                ]
              }
            }
          }
        }
      }
    }
  },
  "transient" : { }
}

PUT /_cluster/settings
{
    "persistent" : {
        "cluster.routing.allocation.awareness.attributes": null,
        "cluster.routing.allocation.awareness.force.az.values": null
    }
}


# Configure the cluster to reflect a hot/warm architecture, with `node1` as the only hot node
add below config to elasticsearch.yml
node1:
node.attr.hot_warm_type: hot

node2,node3
node.attr.hot_warm_type: warm

GET /_cat/nodeattrs?v
node  host      ip        attr            value
node2 10.0.0.13 10.0.0.13 az              earth
node2 10.0.0.13 10.0.0.13 hot_warm_type   warm
node2 10.0.0.13 10.0.0.13 xpack.installed true
node1 10.0.0.12 10.0.0.12 az              earth
node1 10.0.0.12 10.0.0.12 hot_warm_type   hot
node1 10.0.0.12 10.0.0.12 xpack.installed true
node3 10.0.0.14 10.0.0.14 az              mars
node3 10.0.0.14 10.0.0.14 hot_warm_type   warm
node3 10.0.0.14 10.0.0.14 xpack.installed true

DELETE hamlet-1
PUT hamlet-1
{
  "settings": {
    "index.routing.allocation.include.hot_warm_type": "warm"
  }
}

GET _cat/shards/hamlet-1?v
index    shard prirep state   docs store ip        node
hamlet-1 0     p      STARTED    0  230b 10.0.0.14 node3
hamlet-1 0     r      STARTED    0  230b 10.0.0.13 node2

GET /hamlet-1/_settings


PUT hamlet-1/_settings
{
  "settings": {
    "index.routing.allocation.include.hot_warm_type": null
  }
}


# Let's assume that the nodes have either a "large" or "small" local storage. Add the attribute `storage` to the nodes configuration, and set its value so that `node2` is the only with a "small" storage

add below config to elasticsearch.yml
node1,node3:
node.attr.storage: large

node2:
node.attr.storage: small

GET _cat/nodeattrs?v
node  host      ip        attr            value
node2 10.0.0.13 10.0.0.13 xpack.installed true
node2 10.0.0.13 10.0.0.13 storage         small
node1 10.0.0.12 10.0.0.12 xpack.installed true
node1 10.0.0.12 10.0.0.12 storage         large
node3 10.0.0.14 10.0.0.14 xpack.installed true
node3 10.0.0.14 10.0.0.14 storage         large



PUT hamlet-2
{
  "settings": {
    "index.routing.allocation.include.storage": "large"
  }
}

GET _cat/shards/hamlet-2?v
index    shard prirep state   docs store ip        node
hamlet-2 0     r      STARTED    5 5.8kb 10.0.0.14 node3
hamlet-2 0     p      STARTED    5 5.8kb 10.0.0.12 node1

GET hamlet-2/_settings




# Reference
https://www.elastic.co/guide/en/elasticsearch/reference/7.2/index-modules-allocation.html
https://www.elastic.co/guide/en/elasticsearch/reference/7.2/modules-cluster.html
https://www.elastic.co/guide/en/elasticsearch/reference/7.6/indices-create-index.html
https://www.elastic.co/guide/en/elasticsearch/reference/7.6/cluster-reroute.html
