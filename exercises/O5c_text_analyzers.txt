# ** EXAM OBJECTIVE: MAPPINGS AND TEXT ANALYSIS **
# GOAL: Add built-in text analyzers and specify a custom one
# REQUIRED SETUP: (i) a running Elasticsearch cluster with at least one node and a Kibana instance, (ii) no index or index template that starts by `hamlet`
# Copy-paste the following instructions into your Kibana console, and work directly from there

# Create the index `hamlet_1` with one primary shard and no replicas
# Define a mapping for the default type "_doc" of `hamlet_1`, so that (i) the type has three fields, named `speaker`, `line_number`, and `text_entry`, (ii) `text_entry` is associated with the built-in "english" analyzer
# Add some documents to `hamlet_1` by running the following _bulk command
PUT hamlet_1/_doc/_bulk
{"index":{"_index":"hamlet_1","_id":0}}
{"line_number":"1.1.1","speaker":"BERNARDO","text_entry":"Whos there?"}
{"index":{"_index":"hamlet_1","_id":1}}
{"line_number":"1.1.2","speaker":"FRANCISCO","text_entry":"Nay, answer me: stand, and unfold yourself."}
{"index":{"_index":"hamlet_1","_id":2}}
{"line_number":"1.1.3","speaker":"BERNARDO","text_entry":"Long live the king!"}
{"index":{"_index":"hamlet_1","_id":3}}
{"line_number":"1.2.1","speaker":"KING CLAUDIUS","text_entry":"Though yet of Hamlet our dear brothers death"}
# Create the index `hamlet_2` with one primary shard and no replicas
# Add to `hamlet_2` a custom analyzer named `shy_hamlet_analyzer`, which is composed of (i) a char filter to replace the characters "Hamlet" with "[censored]", (ii) a tokenizer to split tokens on whitespaces and columns, (iii) a token filter to ignore any token with less than 5 characters
# Define a mapping for the default type "_doc" of `hamlet_2`, so that (i) the type has one field named `text_entry`, (ii) `text_entry` is associated with the `shy_hamlet_analyzer` created in the previous step
# Reindex the `text_entry` field of `hamlet_1` into `hamlet_2`
# Verify that documents have been reindexed to `hamlet_2` as expected - e.g., by searching for "censored" into the `text_entry` field


#LAB STEPS

DELETE hamlet*
DELETE _template/hamlet*

# Create the index `hamlet_1` with one primary shard and no replicas
# Define a mapping for the default type "_doc" of `hamlet_1`, so that (i) the type has three fields, named `speaker`, `line_number`, and `text_entry`, (ii) `text_entry` is associated with the built-in "english" analyzer
# Add some documents to `hamlet_1` by running the following _bulk command
PUT hamlet_1/_doc/_bulk
{"index":{"_index":"hamlet_1","_id":0}}
{"line_number":"1.1.1","speaker":"BERNARDO","text_entry":"Whos there?"}
{"index":{"_index":"hamlet_1","_id":1}}
{"line_number":"1.1.2","speaker":"FRANCISCO","text_entry":"Nay, answer me: stand, and unfold yourself."}
{"index":{"_index":"hamlet_1","_id":2}}
{"line_number":"1.1.3","speaker":"BERNARDO","text_entry":"Long live the king!"}
{"index":{"_index":"hamlet_1","_id":3}}
{"line_number":"1.2.1","speaker":"KING CLAUDIUS","text_entry":"Though yet of Hamlet our dear brothers death"}



PUT hamlet_1
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 0
  },
  "mappings": {
    "properties": {
      "speaker":{"type": "text"},
      "line_number":{"type": "text"},
      "text_entry":{"type": "text","analyzer": "english"}
    }
  }
}

PUT hamlet_1/_doc/_bulk
{"index":{"_index":"hamlet_1","_id":0}}
{"line_number":"1.1.1","speaker":"BERNARDO","text_entry":"Whos there?"}
{"index":{"_index":"hamlet_1","_id":1}}
{"line_number":"1.1.2","speaker":"FRANCISCO","text_entry":"Nay, answer me: stand, and unfold yourself."}
{"index":{"_index":"hamlet_1","_id":2}}
{"line_number":"1.1.3","speaker":"BERNARDO","text_entry":"Long live the king!"}
{"index":{"_index":"hamlet_1","_id":3}}
{"line_number":"1.2.1","speaker":"KING CLAUDIUS","text_entry":"Though yet of Hamlet our dear brothers death"}


# Create the index `hamlet_2` with one primary shard and no replicas
# Add to `hamlet_2` a custom analyzer named `shy_hamlet_analyzer`, which is composed of (i) a char filter to replace the characters "Hamlet" with "[censored]", (ii) a tokenizer to split tokens on whitespaces and columns, (iii) a token filter to ignore any token with less than 5 characters
# Define a mapping for the default type "_doc" of `hamlet_2`, so that (i) the type has one field named `text_entry`, (ii) `text_entry` is associated with the `shy_hamlet_analyzer` created in the previous step

PUT hamlet_2
{
  "settings": {
    "number_of_replicas": 0,
    "number_of_shards": 1,
    "analysis": {
      "analyzer": {
        "shy_hamlet_analyzer": {
          "type": "custom",
          "tokenizer": "whitespace",
          "char_filter": [
            "my_char_filter"
          ],
          "filter": [
            "my_condition"
          ]
        }},
        "char_filter": {
          "my_char_filter": {
            "type": "mapping",
            "mappings": [
              "Hamlet => [CENSORED]"
            ]
          }
        },
        "filter": {
          "my_condition": {
            "type": "condition",  
            "filter" : [ "lowercase" ],
            "script": {
              "source": "token.getTerm().length() < 5"
            }
          }
        }
      }
  },
  "mappings": {
    "properties": {
      "text_entry":{
        "type": "text",
        "analyzer": "shy_hamlet_analyzer"
      }
    }
  }
}


# Reindex the `text_entry` field of `hamlet_1` into `hamlet_2`
# Verify that documents have been reindexed to `hamlet_2` as expected - e.g., by searching for "censored" into the `text_entry` field

POST _reindex
{
"source": {
  "index": "hamlet_1"
},
"dest": {
  "index": "hamlet_2"
}
}
POST hamlet_2/_search

POST hamlet_2/_analyze
{
  "analyzer": "shy_hamlet_analyzer",
  "text": "Though yet of Hamlet our dear brothers death"
}

POST hamlet_2/_search
{
  "query": {
    "match": {
      "text_entry": "[CENSORED]"
    }
  }
}

#REFERENCE
https://www.elastic.co/guide/en/elasticsearch/reference/7.2/configuring-analyzers.html
https://www.elastic.co/guide/en/elasticsearch/reference/7.2/analysis-lang-analyzer.html
https://www.elastic.co/guide/en/elasticsearch/reference/7.2/analysis-custom-analyzer.html
https://www.elastic.co/guide/en/elasticsearch/reference/7.2/_testing_analyzers.html
https://www.elastic.co/guide/en/elasticsearch/reference/7.2/analysis-whitespace-analyzer.html
https://www.elastic.co/guide/en/elasticsearch/reference/7.2/analysis-whitespace-tokenizer.html
https://www.elastic.co/guide/en/elasticsearch/reference/7.2/analysis-condition-tokenfilter.htmls
